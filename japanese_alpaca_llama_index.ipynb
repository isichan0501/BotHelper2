{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isichan0501/BotHelper/blob/main/japanese_alpaca_llama_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#文字コードエラーでcolobが動かない時に\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "zRTbJ0QxfCiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 認証のためのコード\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# データの永続化\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/My Drive/\"\n",
        "\n",
        "#check exist data\n",
        "import os\n",
        "if not os.path.exists(\"alpaca_llama_index\"):\n",
        "  !git clone https://github.com/isichan0501/alpaca_llama_index.git\n",
        "%cd \"alpaca_llama_index\"\n",
        "\n",
        "\n",
        "# if not os.path.exists(\"japanese-alpaca-lora\"):\n",
        "#   !git clone https://github.com/masa3141/japanese-alpaca-lora.git\n",
        "# train_data_filepath = \"japanese-alpaca-lora/data/japanese_alpaca_data.json\""
      ],
      "metadata": {
        "id": "I4qqMh5hfDY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パッケージのインストール\n",
        "!pip install llama-index\n",
        "!pip install -q bitsandbytes datasets accelerate loralib sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main \n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n"
      ],
      "metadata": {
        "id": "eW9BPgTyRtaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LangchainEmbedding\n",
        "from llama_index import SimpleDirectoryReader, LLMPredictor, PromptHelper, GPTSimpleVectorIndex\n",
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, LLaMAForCausalLM, LLaMATokenizer\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, get_peft_model_state_dict\n",
        "\n",
        "hf_model_path = \"decapoda-research/llama-7b-hf\"\n",
        "alpaca_model_path = \"data/japanese-lora-alpaca\""
      ],
      "metadata": {
        "id": "rOc0Bp5wQFtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run llama-index script\n",
        "\n"
      ],
      "metadata": {
        "id": "GWOX29xYQOqe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2IG8m5ge_Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alpaca-japaneseをダウンロード\n",
        "# Import the necessary libraries\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Enter the shared link for the folder you want to download\n",
        "shared_link = 'https://drive.google.com/drive/folders/1e7Qel8_fSwvpC1utoyGwAItgpcKBL_90?usp=share_link'\n",
        "\n",
        "# Get the ID of the folder from the shared link\n",
        "folder_id = shared_link.split('/')[-2]\n",
        "\n",
        "# Create a URL to download the folder as a zip file\n",
        "url = f'https://drive.google.com/uc?id={folder_id}&export=download'\n",
        "\n",
        "# Send a request to download the folder as a zip file\n",
        "r = requests.get(url)\n",
        "\n",
        "# Unzip the downloaded file\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall(alpaca_model_path)\n"
      ],
      "metadata": {
        "id": "jhjNIIeMeS-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(hf_model_path)\n",
        "\n",
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    hf_model_path,\n",
        "    load_in_8bit=True, #Dissabling could solve some errors\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, alpaca_model_path)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "max_length = 1500 #2048\n",
        "max_new_tokens = 48\n",
        "\n",
        "\n",
        "class LLaMALLM(LLM):\n",
        "    def _call(self, prompt, stop=None):\n",
        "        prompt += \"### Response:\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].cuda()\n",
        "        \n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=0.6,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.15,\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            generation_output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=128,\n",
        "            )\n",
        "        response = \"\"\n",
        "        for s in generation_output.sequences:\n",
        "            response += tokenizer.decode(s)\n",
        "            \n",
        "        response = response[len(prompt):]\n",
        "        print(\"Model Response:\", response)\n",
        "        return response\n",
        "\n",
        "    def _identifying_params(self):\n",
        "        return {\"name_of_model\": \"alpaca\"}\n",
        "\n",
        "    def _llm_type(self):\n",
        "        return \"custom\"\n",
        "\n",
        "max_input_size = max_length\n",
        "num_output = max_new_tokens\n",
        "max_chunk_overlap = 20"
      ],
      "metadata": {
        "id": "R0jT_ze7Q2Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "llm_predictor = LLMPredictor(llm=LLaMALLM())\n",
        "index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper)\n",
        "\n"
      ],
      "metadata": {
        "id": "WkmM5-_9RJdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.save_to_disk('index.json')\n",
        "new_index = GPTSimpleVectorIndex.load_from_disk('index.json', embed_model=embed_model, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "\n",
        "response = new_index.query(\"ぼっちちゃんの得意な楽器は？\")\n",
        "print(response.response)\n",
        "\n",
        "response = new_index.query(\"ぼっちちゃんについてどう思う？\")\n",
        "print(response.response)\n",
        "\n",
        "response = new_index.query(\"喜多ちゃんの一番中の良い友達は？\")\n",
        "print(response.response)"
      ],
      "metadata": {
        "id": "Mdm1TNiGRGNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "weYOCOJzQIeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbLLb7R-lzpa"
      },
      "outputs": [],
      "source": [
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = LLaMATokenizer.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\", add_eos_token=True\n",
        ")\n",
        "\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwUS7bg2l4s2"
      },
      "outputs": [],
      "source": [
        "# Please upload japanese_alpaca_data.json\n",
        "data = load_dataset(\"json\", data_files=\"japanese_alpaca_data.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n36u9r9mAjU"
      },
      "outputs": [],
      "source": [
        "# Split data set into train and validation data.\n",
        "train_val = data[\"train\"].train_test_split(\n",
        "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
        ")\n",
        "train_data = train_val[\"train\"]\n",
        "val_data = train_val[\"test\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7r-CM42mQEk"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    # sorry about the formatting disaster gotta move fast\n",
        "    if data_point[\"input\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "\n",
        "\n",
        "def tokenize(prompt):\n",
        "    # there's probably a way to do this with the tokenizer settings\n",
        "    # but again, gotta move fast\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN + 1,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": result[\"input_ids\"][:-1],\n",
        "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
        "    }\n",
        "\n",
        "\n",
        "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)))\n",
        "val_data = val_data.shuffle().map(lambda x: tokenize(generate_prompt(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AUhF_Q0ma1S"
      },
      "outputs": [],
      "source": [
        "world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
        "ddp = world_size != 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRZ7XjHEmUzQ"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=20,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_steps=200,\n",
        "        output_dir=\"japanese-lora-alpaca\",\n",
        "        save_total_limit=3,\n",
        "        load_best_model_at_end=True,\n",
        "        ddp_find_unused_parameters=False if ddp else None,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsivWRBXmpN_"
      },
      "outputs": [],
      "source": [
        "old_state_dict = model.state_dict\n",
        "model.state_dict = (\n",
        "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
        ").__get__(model, type(model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GppGXQpomt_t"
      },
      "outputs": [],
      "source": [
        "# Start finetunig. It takes around 7 hours on Google Colab PRO+.\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dft7Tb_7mvph"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"japanese-lora-alpaca\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNU4xqrWnIC-"
      },
      "source": [
        "## Load finetuned model and check the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy4lVW9knSjL"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aBMrwdYnNOU"
      },
      "outputs": [],
      "source": [
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "model_custom = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model_original = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model_custom = PeftModel.from_pretrained(model_custom, \"japanese-lora-alpaca\")\n",
        "model_original = PeftModel.from_pretrained(model_original, \"tloen/alpaca-lora-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Muay10MFnjWx"
      },
      "outputs": [],
      "source": [
        "def generate_instruction_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TNicOPXnkRo"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def evaluate(model_aaa, instruction, input=None):\n",
        "    prompt = generate_instruction_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model_aaa.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Response:\", output.split(\"### Response:\")[1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCxBdnG1npa7"
      },
      "outputs": [],
      "source": [
        "evaluate(model_custom,\"日本の首都はどこですか？\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRc_AY8jnukE"
      },
      "outputs": [],
      "source": [
        "evaluate(model_original,\"日本の首都はどこですか？\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}